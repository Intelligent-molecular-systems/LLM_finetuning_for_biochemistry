{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is used to train a XGBoost model as a baseline for EC number/product/susbtrate prediction in low-data regimes.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "from rdkit.Chem import rdChemReactions\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import TrainingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_parse_data(file_path, task_type):\n",
    "    \"\"\"\n",
    "    Load and parse the JSON data based on the task type (ec_prediction, product_prediction, substrate_prediction).\n",
    "    \"\"\"\n",
    "    # Load the JSON data\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Create lists for processed data\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    raw_inputs = []\n",
    "    raw_outputs = []\n",
    "    ec_numbers = []  # For product/substrate prediction\n",
    "\n",
    "    for entry in data:\n",
    "        raw_input = entry['raw_input']\n",
    "        raw_output = entry['raw_output']\n",
    "\n",
    "        if task_type == 'ec_prediction':\n",
    "            # For EC prediction, use the full reaction string \"reactant>>product\"\n",
    "            if \">>\" not in raw_input:\n",
    "                continue  # Skip invalid rows\n",
    "            \n",
    "            raw_inputs.append(raw_input)  # Store full \"reactant>>product\" string\n",
    "            inputs.append(raw_input)  # Used directly for reaction fingerprinting\n",
    "            outputs.append(raw_output)  # EC number\n",
    "            raw_outputs.append(raw_output)  # Store raw EC number\n",
    "\n",
    "        elif task_type in ['product_prediction', 'substrate_prediction']:\n",
    "            # For Product/Substrate prediction, separate SMILES and EC from raw input\n",
    "            if \"|\" not in raw_input:\n",
    "                continue  # Skip invalid rows\n",
    "\n",
    "            smiles, ec_number = raw_input.split('|')\n",
    "            raw_inputs.append(raw_input)  # Store full input\n",
    "            inputs.append(smiles)  # Used for molecular fingerprinting\n",
    "            outputs.append(raw_output)  # Predicting SMILES\n",
    "            raw_outputs.append(raw_output)  # Store expected SMILES output\n",
    "            ec_numbers.append(ec_number)  # Store EC separately for encoding\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'input': inputs,\n",
    "        'output': outputs,\n",
    "        'raw_input': raw_inputs,\n",
    "        'raw_output': raw_outputs\n",
    "    })\n",
    "\n",
    "    # Add EC number column for product/substrate prediction\n",
    "    if task_type in ['product_prediction', 'substrate_prediction']:\n",
    "        df['ec_number'] = ec_numbers\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EC prediction dataset (train and test sets)\n",
    "ec_train = load_and_parse_data(\"Data_json/ec_train_set.json\", task_type='ec_prediction')\n",
    "ec_test = load_and_parse_data(\"Data_json/ec_test_set.json\", task_type='ec_prediction')\n",
    "\n",
    "# Load Product prediction dataset (train and test sets)\n",
    "product_train = load_and_parse_data(\"Data_json/product_train_set.json\", task_type='product_prediction')\n",
    "product_test = load_and_parse_data(\"Data_json/product_test_set.json\", task_type='product_prediction')\n",
    "\n",
    "# Load Substrate prediction dataset (train and test sets)\n",
    "substrate_train = load_and_parse_data(\"Data_json/substrate_train_set.json\", task_type='substrate_prediction')\n",
    "substrate_test = load_and_parse_data(\"Data_json/substrate_test_set.json\", task_type='substrate_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for the three tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Encoding & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ec_levels(df, column_name='raw_output'):\n",
    "    \"\"\"Extract hierarchical EC levels (L1, L2, L3, L4) from a given column.\"\"\"\n",
    "    df[['L1', 'L2', 'L3', 'L4']] = df[column_name].str.split('.', expand=True).astype(float)\n",
    "    return df\n",
    "\n",
    "def fit_ec_encoders_and_scaler(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Fit LabelEncoders and StandardScaler using both train & test EC numbers.\n",
    "    Returns trained encoders, scaler, the transformed DataFrames, and a lookup dictionary.\n",
    "    \"\"\"\n",
    "    # Merge train & test before encoding\n",
    "    combined_ec = pd.concat([train_df[['L1', 'L2', 'L3', 'L4']], \n",
    "                             test_df[['L1', 'L2', 'L3', 'L4']]], \n",
    "                             axis=0).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Fit LabelEncoders ONCE\n",
    "    encoders = {col: LabelEncoder().fit(combined_ec[col]) for col in ['L1', 'L2', 'L3', 'L4']}\n",
    "\n",
    "    # Apply Label Encoding (Copy DataFrames to Avoid Overwriting)\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    for col in ['L1', 'L2', 'L3', 'L4']:\n",
    "        train_df[col] = encoders[col].transform(train_df[col])\n",
    "        test_df[col] = encoders[col].transform(test_df[col])\n",
    "\n",
    "    # Create Lookup Table BEFORE Standardization\n",
    "    combined_encoded = pd.concat([train_df[['L1', 'L2', 'L3', 'L4']], \n",
    "                                  test_df[['L1', 'L2', 'L3', 'L4']]], \n",
    "                                  axis=0).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    ec_lookup_dict = {tuple(row): ec for row, ec in zip(combined_encoded.values, combined_ec.values)}\n",
    "    scaler = StandardScaler().fit(train_df[['L1', 'L2', 'L3', 'L4']])\n",
    "\n",
    "    # Apply Standardization\n",
    "    train_df[['L1', 'L2', 'L3', 'L4']] = scaler.transform(train_df[['L1', 'L2', 'L3', 'L4']])\n",
    "    test_df[['L1', 'L2', 'L3', 'L4']] = scaler.transform(test_df[['L1', 'L2', 'L3', 'L4']])\n",
    "\n",
    "    return train_df, test_df, encoders, scaler, ec_lookup_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Processed Data for Training (for Each Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract EC levels for each task ---\n",
    "ec_train = extract_ec_levels(ec_train, column_name='raw_output')\n",
    "ec_test = extract_ec_levels(ec_test, column_name='raw_output')\n",
    "\n",
    "product_train = extract_ec_levels(product_train, column_name='ec_number')\n",
    "product_test = extract_ec_levels(product_test, column_name='ec_number')\n",
    "\n",
    "substrate_train = extract_ec_levels(substrate_train, column_name='ec_number')\n",
    "substrate_test = extract_ec_levels(substrate_test, column_name='ec_number')\n",
    "\n",
    "# --- Apply Unified EC Encoding ---\n",
    "ec_train, ec_test, ec_encoders, ec_scaler, ec_lookup_dict = fit_ec_encoders_and_scaler(ec_train, ec_test)\n",
    "product_train, product_test, product_encoders, product_scaler, product_lookup_dict = fit_ec_encoders_and_scaler(product_train, product_test)\n",
    "substrate_train, substrate_test, substrate_encoders, substrate_scaler, substrate_lookup_dict = fit_ec_encoders_and_scaler(substrate_train, substrate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_morgan(smiles, radius=2, n_bits=256):\n",
    "    \"\"\"Convert a SMILES string into a Morgan fingerprint using the newer MorganGenerator API.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(n_bits)  # Return a zero vector for invalid SMILES\n",
    "    \n",
    "    # Use the new MorganGenerator API\n",
    "    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "    return np.array(generator.GetFingerprint(mol))\n",
    "\n",
    "\n",
    "\n",
    "def reaction_to_fingerprint(reaction_smiles, bit_vector_size=1024):\n",
    "    \"\"\"\n",
    "    Generates a reaction fingerprint from a reaction SMILES and returns it as a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "        reaction_smiles (str): Reaction SMILES string.\n",
    "        bit_vector_size (int): Size of the bit vector (default: 2048).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Reaction fingerprint as a NumPy array of shape (bit_vector_size,).\n",
    "    \"\"\"\n",
    "    # Convert SMILES to an RDKit reaction object\n",
    "    reaction = rdChemReactions.ReactionFromSmarts(reaction_smiles, useSmiles=True)\n",
    "    \n",
    "    # Generate the reaction fingerprint (returns UIntSparseIntVect)\n",
    "    rxn_fp = rdChemReactions.CreateDifferenceFingerprintForReaction(reaction)\n",
    "    \n",
    "    # Initialize a zeroed NumPy array of desired size\n",
    "    fingerprint_array = np.zeros(bit_vector_size, dtype=np.uint8)\n",
    "\n",
    "    # Get the nonzero bit positions from the fingerprint\n",
    "    active_bits = rxn_fp.GetNonzeroElements().keys()\n",
    "\n",
    "    # Set the corresponding positions in the NumPy array to 1\n",
    "    for bit in active_bits:\n",
    "        if bit < bit_vector_size:  # Ensure we don't exceed the array size\n",
    "            fingerprint_array[bit] = 1\n",
    "\n",
    "    return np.array(fingerprint_array)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features(df, task):\n",
    "    \"\"\"Prepare input-output data for XGBoost based on task.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if task == \"ec_prediction\":\n",
    "        \n",
    "        # Inputs: Reaction fingerprint (reactant >> product)\n",
    "        df['rxn_fp'] = df['input'].apply(reaction_to_fingerprint)\n",
    "        \n",
    "        # Concatenate EC hierarchical encoding\n",
    "        df['features'] = df['rxn_fp']\n",
    "\n",
    "        X = np.vstack(df['features'].values)\n",
    "        y = df[['L1', 'L2', 'L3', 'L4']].values  # Multi-output for EC hierarchy\n",
    "\n",
    "    elif task in [\"product_prediction\", \"substrate_prediction\"]:\n",
    "        # Inputs: Reactant/Product + EC hierarchical encoding\n",
    "        df['mol_fp'] = df['input'].apply(smiles_to_morgan)\n",
    "\n",
    "        # Concatenate EC hierarchical encoding\n",
    "        df['features'] = df.apply(lambda row: np.concatenate([row['mol_fp'], row[['L1', 'L2', 'L3', 'L4']].values]), axis=1)\n",
    "\n",
    "        X = np.vstack(df['features'].values)\n",
    "        y = np.vstack(df['output'].apply(smiles_to_morgan).values)  # Predicting Morgan Fingerprint\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def inverse_transform_ec(y_standardized, scaler):\n",
    "    \"\"\"\n",
    "    Convert standardized EC encoding predictions back to the closest known EC number.\n",
    "    \"\"\"\n",
    "    y_original = scaler.inverse_transform(y_standardized)  # De-standardize\n",
    "    y_rounded = np.rint(y_original).astype(int)  # Round to nearest integer\n",
    "    return y_rounded\n",
    "\n",
    "\n",
    "\n",
    "class TQDMProgressBar(TrainingCallback):\n",
    "    \"\"\"Custom callback for XGBoost training progress visualization.\"\"\"\n",
    "    \n",
    "    def __init__(self, total_rounds):\n",
    "        self.progress_bar = tqdm(total=total_rounds, desc=\"Training Progress\", position=0, leave=True)\n",
    "    \n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        \"\"\"Update progress bar with latest validation metric.\"\"\"\n",
    "        if evals_log:\n",
    "            metric_name = list(evals_log[\"validation_0\"].keys())[0]  # Get metric name\n",
    "            latest_score = evals_log[\"validation_0\"][metric_name][-1]  # Get latest score\n",
    "            self.progress_bar.set_postfix({metric_name: latest_score})  # Update bar\n",
    "        \n",
    "        self.progress_bar.update(1)\n",
    "        return False  # Continue training\n",
    "    \n",
    "    def after_training(self, model):\n",
    "        \"\"\"Close progress bar and return model.\"\"\"\n",
    "        self.progress_bar.close()\n",
    "        return model  # ✅ Ensure callback properly returns model\n",
    "    \n",
    "\n",
    "\n",
    "def convert_hierarchical_to_ec(hierarchical_encoding, lookup_dict):\n",
    "    \"\"\"\n",
    "    Convert hierarchical encoding back to the closest known EC number.\n",
    "    \n",
    "    Parameters:\n",
    "    - hierarchical_encoding: Tuple of (L1, L2, L3, L4) values.\n",
    "    - lookup_dict: Dictionary mapping hierarchical encodings back to EC numbers.\n",
    "\n",
    "    Returns:\n",
    "    - Closest EC number as a string.\n",
    "    \"\"\"\n",
    "    hierarchical_encoding = tuple(np.round(hierarchical_encoding).astype(int))  # Round to nearest int\n",
    "    return hierarchical_encoding\n",
    "\n",
    "\n",
    "\n",
    "def compute_ec_accuracy(y_test_ec, y_pred_ec):\n",
    "    \"\"\"\n",
    "    Compute accuracy at each EC hierarchy level (L1, L1+L2, L1+L2+L3, Full L1+L2+L3+L4).\n",
    "\n",
    "    Parameters:\n",
    "    - y_test_ec: List of actual EC numbers (each as a list of 4 integers).\n",
    "    - y_pred_ec: List of predicted EC numbers (each as a list of 4 integers).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with accuracy at each EC level.\n",
    "    \"\"\"\n",
    "    print(y_test_ec[:5])\n",
    "    print(y_pred_ec[:5])\n",
    "    correct_L1 = 0\n",
    "    correct_L2 = 0\n",
    "    correct_L3 = 0\n",
    "    correct_L4 = 0\n",
    "    total = len(y_test_ec)\n",
    "\n",
    "    for true_ec, pred_ec in zip(y_test_ec, y_pred_ec):\n",
    "        if true_ec[0] == pred_ec[0]:  # L1 match\n",
    "            correct_L1 += 1\n",
    "        if (true_ec[0] == pred_ec[0]) and (true_ec[1] == pred_ec[1]):  # L1 + L2 match\n",
    "            correct_L2 += 1\n",
    "        if (true_ec[0] == pred_ec[0]) and (true_ec[1] == pred_ec[1]) and (true_ec[2] == pred_ec[2]):  # L1 + L2 + L3 match\n",
    "            correct_L3 += 1\n",
    "        if (true_ec[0] == pred_ec[0]) and (true_ec[1] == pred_ec[1]) and (true_ec[2] == pred_ec[2]) and (true_ec[3] == pred_ec[3]):  # Full EC match\n",
    "            correct_L4 += 1\n",
    "\n",
    "    return {\n",
    "        \"L1 Accuracy\": correct_L1 / total,\n",
    "        \"L1+L2 Accuracy\": correct_L2 / total,\n",
    "        \"L1+L2+L3 Accuracy\": correct_L3 / total,\n",
    "        \"Full EC Accuracy\": correct_L4 / total\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def compute_ec_micro_avg_accuracy(y_test_ec, y_pred_ec):\n",
    "    \"\"\"\n",
    "    Compute micro-averaged accuracy at each EC hierarchy level (L1, L1+L2, L1+L2+L3, Full L1+L2+L3+L4)\n",
    "    **per L1 class**, then average across all encountered L1 classes.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test_ec: List of actual EC numbers (each as a list of 4 integers).\n",
    "    - y_pred_ec: List of predicted EC numbers (each as a list of 4 integers).\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with micro-averaged accuracy at each EC level.\n",
    "    \"\"\"\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Store counts per L1 class\n",
    "    classwise_correct = defaultdict(lambda: {\"L1\": 0, \"L2\": 0, \"L3\": 0, \"L4\": 0, \"total\": 0})\n",
    "\n",
    "    # Populate accuracy per L1 class\n",
    "    for true_ec, pred_ec in zip(y_test_ec, y_pred_ec):\n",
    "        main_class = true_ec[0]  # L1 class identifier\n",
    "        classwise_correct[main_class][\"total\"] += 1  # Count instances\n",
    "\n",
    "        if true_ec[0] == pred_ec[0]:  # L1 match\n",
    "            classwise_correct[main_class][\"L1\"] += 1\n",
    "        if (true_ec[0] == pred_ec[0]) and (true_ec[1] == pred_ec[1]):  # L1 + L2 match\n",
    "            classwise_correct[main_class][\"L2\"] += 1\n",
    "        if (true_ec[0] == pred_ec[0]) and (true_ec[1] == pred_ec[1]) and (true_ec[2] == pred_ec[2]):  # L1 + L2 + L3 match\n",
    "            classwise_correct[main_class][\"L3\"] += 1\n",
    "        if (true_ec[0] == pred_ec[0]) and (true_ec[1] == pred_ec[1]) and (true_ec[2] == pred_ec[2]) and (true_ec[3] == pred_ec[3]):  # Full EC match\n",
    "            classwise_correct[main_class][\"L4\"] += 1\n",
    "\n",
    "    # Compute micro-averaged accuracy per class\n",
    "    accuracies = {\"L1 Accuracy\": 0, \"L1+L2 Accuracy\": 0, \"L1+L2+L3 Accuracy\": 0, \"Full EC Accuracy\": 0}\n",
    "    encountered_classes = len(classwise_correct)\n",
    "\n",
    "    for main_class, counts in classwise_correct.items():\n",
    "        if counts[\"total\"] > 0:  # Avoid division by zero\n",
    "            accuracies[\"L1 Accuracy\"] += counts[\"L1\"] / counts[\"total\"]\n",
    "            accuracies[\"L1+L2 Accuracy\"] += counts[\"L2\"] / counts[\"total\"]\n",
    "            accuracies[\"L1+L2+L3 Accuracy\"] += counts[\"L3\"] / counts[\"total\"]\n",
    "            accuracies[\"Full EC Accuracy\"] += counts[\"L4\"] / counts[\"total\"]\n",
    "\n",
    "    # Average over all encountered L1 classes\n",
    "    for key in accuracies:\n",
    "        accuracies[key] /= encountered_classes\n",
    "\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X, y, scaler, train_size=2000, task = 'ec_encoding', num_boost_round=100, lookup_dict=None, scale_ec_factor = None):\n",
    "    \"\"\"\n",
    "    Train and evaluate an XGBoost model for EC prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input reaction fingerprints.\n",
    "    - y: 4D hierarchical EC encoding.\n",
    "    - scaler: StandardScaler used for EC encoding.\n",
    "    - num_boost_round: Maximum boosting rounds.\n",
    "    - ec_lookup_dict: Dictionary mapping hierarchical encodings back to actual EC numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split Data (Ensuring Validation Set Stays Fixed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=42)\n",
    "    X_train, y_train = X_train[:train_size], y_train[:train_size]  # Allow smaller training set\n",
    "\n",
    "    # EC Prediction (Reaction → EC Encoding)\n",
    "    if task == \"ec_prediction\":\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            eval_metric=\"rmse\",\n",
    "            early_stopping_rounds=10,\n",
    "            callbacks=[TQDMProgressBar(num_boost_round)]\n",
    "        )\n",
    "\n",
    "        # Train Model and Store Evaluation Results\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],  # Validate only on test set\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Predict Standardized EC Encodings\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Convert Predictions Back to Hierarchical Encoding\n",
    "        y_test_hierarchical = scaler.inverse_transform(y_test)\n",
    "        y_pred_hierarchical = scaler.inverse_transform(y_pred)\n",
    "\n",
    "        # Convert Hierarchical Encoding to Actual EC Numbers Using Lookup Table\n",
    "        if lookup_dict is not None:\n",
    "            y_test_ec = [convert_hierarchical_to_ec(tuple(vec), lookup_dict) for vec in y_test_hierarchical]\n",
    "            y_pred_ec = [convert_hierarchical_to_ec(tuple(vec), lookup_dict) for vec in y_pred_hierarchical]\n",
    "        else:\n",
    "            y_test_ec = []\n",
    "            y_pred_ec = []\n",
    "\n",
    "        return model, model.evals_result(), y_test_hierarchical, y_pred_hierarchical, y_test_ec, y_pred_ec\n",
    "\n",
    "\n",
    "    # Product/Substrate Prediction (Molecule FP + EC Encoding → Molecule FP)\n",
    "    else:\n",
    "        # **Rescale EC Encoding in Input Features**\n",
    "        if scale_ec_factor is not None:\n",
    "            X_train[:, -4:] *= scale_ec_factor\n",
    "            X_test[:, -4:] *= scale_ec_factor\n",
    "        else:\n",
    "            X_train = X_train[:, :-4] # Remove EC features\n",
    "            X_test = X_test[:, :-4]\n",
    "\n",
    "        # Use `logloss` if treating each bit as a separate binary class\n",
    "        model = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",  # Logistic regression per bit\n",
    "            eval_metric=\"logloss\",\n",
    "            early_stopping_rounds=10,\n",
    "            callbacks=[TQDMProgressBar(num_boost_round)]\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Predict and Compute Accuracy\n",
    "        y_pred_prob = model.predict_proba(X_test)  # Get probability output\n",
    "        y_pred_binary = (y_pred_prob > 0.5).astype(int)  # Convert to binary (threshold at 0.5)\n",
    "\n",
    "        fingerprint_accuracy = np.mean(np.all(y_pred_binary == y_test, axis=1))\n",
    "        print(f\"{task} - Fingerprint Accuracy: {fingerprint_accuracy:.4f}\")\n",
    "\n",
    "        return model, model.evals_result(), y_test, y_pred_binary\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def test_xgboost(model, X_test, y_test, scaler, lookup_dict=None, task=\"ec_prediction\", scale_ec_factor = None):\n",
    "    \"\"\"\n",
    "    Validate a trained XGBoost model on a held-out test set.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained XGBoost model.\n",
    "    - X_test: Features of held-out test set.\n",
    "    - y_test: True labels (Hierarchical EC encodings or Morgan Fingerprints).\n",
    "    - scaler: StandardScaler used during training (only for EC prediction).\n",
    "    - lookup_dict: Dictionary to map hierarchical encodings back to EC numbers (only for EC prediction).\n",
    "    - task: Task type ('ec_prediction' or 'product_prediction'/'substrate_prediction').\n",
    "    \"\"\"\n",
    "\n",
    "    # 1EC PREDICTION (Reaction → EC Encoding)\n",
    "    if task == \"ec_prediction\":\n",
    "        # Predict standardized hierarchical encodings\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Convert back to original hierarchical encodings\n",
    "        y_test_hierarchical = scaler.inverse_transform(y_test)\n",
    "        y_pred_hierarchical = scaler.inverse_transform(y_pred)\n",
    "\n",
    "        # Convert hierarchical encodings back to EC numbers\n",
    "        y_test_ec = [convert_hierarchical_to_ec(tuple(vec), lookup_dict) for vec in y_test_hierarchical]\n",
    "        y_pred_ec = [convert_hierarchical_to_ec(tuple(vec), lookup_dict) for vec in y_pred_hierarchical]\n",
    "\n",
    "        # Compute accuracy at each EC level\n",
    "        ec_accuracy = compute_ec_accuracy(y_test_ec, y_pred_ec)\n",
    "        ec_micro_accuracy = compute_ec_micro_avg_accuracy(y_test_ec, y_pred_ec)\n",
    "        print()\n",
    "        print('########### test micro-average', ec_micro_accuracy)\n",
    "\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nXGBoost EC Prediction Accuracy on Held-Out Test Set:\")\n",
    "        for level, acc in ec_accuracy.items():\n",
    "            print(f\"{level}: {acc:.4f}\")\n",
    "\n",
    "        return ec_accuracy\n",
    "\n",
    "    # PRODUCT/SUBSTRATE PREDICTION (Molecule FP + EC Encoding → Molecule FP)\n",
    "    else:\n",
    "        if scale_ec_factor is not None:\n",
    "            X_test[:, -4:] *= scale_ec_factor\n",
    "        else:\n",
    "            X_test = X_test[:, :-4]  # Remove EC encoding from input feature\n",
    "        # Predict fingerprint bit vectors\n",
    "        y_pred_prob = model.predict_proba(X_test)  # Get probability outputs per bit\n",
    "        y_pred_binary = (y_pred_prob > 0.5).astype(int)  # Convert to binary (threshold at 0.5)\n",
    "\n",
    "        # Compute exact fingerprint match accuracy\n",
    "        fingerprint_accuracy = np.mean(np.all(y_pred_binary == y_test, axis=1))\n",
    "\n",
    "        print(f\"\\nXGBoost {task} Accuracy on Held-Out Test Set:\")\n",
    "        print(f\"Exact Fingerprint Match Accuracy: {fingerprint_accuracy:.4f}\")\n",
    "\n",
    "        return fingerprint_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for all three tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing EC Prediction Data...\n",
      "Preparing Product Prediction Data...\n",
      "Preparing Substrate Prediction Data...\n",
      "Preparing EC Prediction Data...\n",
      "Preparing Product Prediction Data...\n",
      "Preparing Substrate Prediction Data...\n",
      "\n",
      "Training EC Prediction Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  51%|█████     | 51/100 [00:00<00:00, 83.61it/s, rmse=0.67] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0, 2, 24), (1, 0, 0, 139), (0, 2, 0, 71), (0, 12, 10, 170), (1, 4, 0, 81)]\n",
      "[(2, 4, 2, 25), (1, 0, 0, 151), (0, 12, 9, 12), (0, 5, 3, 134), (1, 4, 1, 78)]\n",
      "L1 Accuracy: 0.8090\n",
      "L1+L2 Accuracy: 0.3467\n",
      "L1+L2+L3 Accuracy: 0.2161\n",
      "Full EC Accuracy: 0.0000\n",
      "[(0, 1, 0, 80), (2, 0, 1, 19), (1, 0, 0, 217), (0, 0, 0, 0), (3, 0, 0, 75)]\n",
      "[(0, 2, 1, 117), (2, 0, 1, 21), (1, -1, 0, 176), (0, 1, 1, 93), (2, 1, 1, 14)]\n",
      "\n",
      "########### test micro-average {'L1 Accuracy': 0.5397754713322064, 'L1+L2 Accuracy': 0.23710122922301835, 'L1+L2+L3 Accuracy': 0.1592114018355814, 'Full EC Accuracy': 0.0}\n",
      "\n",
      "XGBoost EC Prediction Accuracy on Held-Out Test Set:\n",
      "L1 Accuracy: 0.7485\n",
      "L1+L2 Accuracy: 0.2772\n",
      "L1+L2+L3 Accuracy: 0.1719\n",
      "Full EC Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 16/100 [00:00<00:00, 101.73it/s, rmse=0.783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0, 2, 24), (1, 0, 0, 139), (0, 2, 0, 71), (0, 12, 10, 170), (1, 4, 0, 81)]\n",
      "[(2, 3, 3, 42), (1, 1, 1, 104), (1, 10, 14, 33), (0, 2, 2, 87), (2, 3, 1, 71)]\n",
      "L1 Accuracy: 0.6784\n",
      "L1+L2 Accuracy: 0.0603\n",
      "L1+L2+L3 Accuracy: 0.0101\n",
      "Full EC Accuracy: 0.0000\n",
      "[(0, 1, 0, 80), (2, 0, 1, 19), (1, 0, 0, 217), (0, 0, 0, 0), (3, 0, 0, 75)]\n",
      "[(1, 3, 1, 82), (3, 2, 0, 44), (1, 1, 0, 104), (0, 2, 2, 80), (2, 2, 1, 26)]\n",
      "\n",
      "########### test micro-average {'L1 Accuracy': 0.40851211895022504, 'L1+L2 Accuracy': 0.05987364211035982, 'L1+L2+L3 Accuracy': 0.017093603475717294, 'Full EC Accuracy': 0.0}\n",
      "\n",
      "XGBoost EC Prediction Accuracy on Held-Out Test Set:\n",
      "L1 Accuracy: 0.6807\n",
      "L1+L2 Accuracy: 0.1076\n",
      "L1+L2+L3 Accuracy: 0.0339\n",
      "Full EC Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 14/100 [00:00<00:00, 113.23it/s, rmse=0.867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0, 2, 24), (1, 0, 0, 139), (0, 2, 0, 71), (0, 12, 10, 170), (1, 4, 0, 81)]\n",
      "[(0, 4, 7, 18), (2, 2, 1, 180), (1, 10, 12, 48), (0, 3, 5, 77), (1, 3, 1, 70)]\n",
      "L1 Accuracy: 0.5678\n",
      "L1+L2 Accuracy: 0.0503\n",
      "L1+L2+L3 Accuracy: 0.0050\n",
      "Full EC Accuracy: 0.0000\n",
      "[(0, 1, 0, 80), (2, 0, 1, 19), (1, 0, 0, 217), (0, 0, 0, 0), (3, 0, 0, 75)]\n",
      "[(0, 1, 1, 79), (1, 3, 1, 55), (1, 2, 1, 180), (0, 3, 2, 27), (2, 3, 2, 22)]\n",
      "\n",
      "########### test micro-average {'L1 Accuracy': 0.32700450970917977, 'L1+L2 Accuracy': 0.04482406331999828, 'L1+L2+L3 Accuracy': 0.0015786983266658065, 'Full EC Accuracy': 0.0}\n",
      "\n",
      "XGBoost EC Prediction Accuracy on Held-Out Test Set:\n",
      "L1 Accuracy: 0.6082\n",
      "L1+L2 Accuracy: 0.0807\n",
      "L1+L2+L3 Accuracy: 0.0023\n",
      "Full EC Accuracy: 0.0000\n",
      "\n",
      "Training Product Prediction Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  37%|███▋      | 37/100 [00:15<00:25,  2.45it/s, logloss=0.146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_prediction - Fingerprint Accuracy: 0.0914\n",
      "\n",
      "Training Substrate Prediction Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  35%|███▌      | 35/100 [00:14<00:26,  2.48it/s, logloss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substrate_prediction - Fingerprint Accuracy: 0.0704\n",
      "\n",
      "XGBoost product_prediction Accuracy on Held-Out Test Set:\n",
      "Exact Fingerprint Match Accuracy: 0.0513\n",
      "\n",
      "XGBoost substrate_prediction Accuracy on Held-Out Test Set:\n",
      "Exact Fingerprint Match Accuracy: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    \n",
    "    # --- Prepare Data for All Three Tasks ---\n",
    "    print(\"Preparing EC Prediction Data...\")\n",
    "    X_ec, y_ec = prepare_features(ec_train, task=\"ec_prediction\")\n",
    "    print(\"Preparing Product Prediction Data...\")\n",
    "    X_product, y_product = prepare_features(product_train, task=\"product_prediction\")\n",
    "    print(\"Preparing Substrate Prediction Data...\")\n",
    "    X_substrate, y_substrate = prepare_features(substrate_train, task=\"substrate_prediction\")\n",
    "\n",
    "    print(\"Preparing EC Prediction Data...\")\n",
    "    X_ec_test, y_ec_test = prepare_features(ec_test, task=\"ec_prediction\")\n",
    "    print(\"Preparing Product Prediction Data...\")\n",
    "    X_product_test, y_product_test = prepare_features(product_test, task=\"product_prediction\")\n",
    "    print(\"Preparing Substrate Prediction Data...\")\n",
    "    X_substrate_test, y_substrate_test = prepare_features(substrate_test, task=\"substrate_prediction\")\n",
    "\n",
    "\n",
    "    # EC prediction training (1024-bit reaction fingerprint)\n",
    "    print(\"\\nTraining EC Prediction Model...\")\n",
    "    # --- Training Set = 2000 ---\n",
    "    model, evals_result, true, pred, c_true, c_pred = train_xgboost(X=X_ec, y=y_ec, scaler=ec_scaler, train_size=2000, task = 'ec_prediction', lookup_dict = ec_lookup_dict)\n",
    "    ec_accuracy = compute_ec_accuracy(c_true, c_pred)\n",
    "    for level, acc in ec_accuracy.items():\n",
    "        print(f\"{level}: {acc:.4f}\")\n",
    "    test_acc = test_xgboost(model, X_test=X_ec_test, y_test=y_ec_test, scaler=ec_scaler, lookup_dict=ec_lookup_dict, task='ec_prediction')\n",
    "\n",
    "    # --- Training Set = 600 ---\n",
    "    model, evals_result, true, pred, c_true, c_pred = train_xgboost(X=X_ec, y=y_ec, scaler=ec_scaler, train_size=600, task = 'ec_prediction', lookup_dict = ec_lookup_dict)\n",
    "    ec_accuracy = compute_ec_accuracy(c_true, c_pred)\n",
    "    for level, acc in ec_accuracy.items():\n",
    "        print(f\"{level}: {acc:.4f}\")\n",
    "    test_acc = test_xgboost(model, X_test=X_ec_test, y_test=y_ec_test, scaler=ec_scaler, lookup_dict=ec_lookup_dict, task='ec_prediction')\n",
    "\n",
    "\n",
    "    # --- Training Set = 200 ---\n",
    "    model, evals_result, true, pred, c_true, c_pred = train_xgboost(X=X_ec, y=y_ec, scaler=ec_scaler, train_size=200, task = 'ec_prediction', lookup_dict = ec_lookup_dict)\n",
    "    ec_accuracy = compute_ec_accuracy(c_true, c_pred)\n",
    "    for level, acc in ec_accuracy.items():\n",
    "        print(f\"{level}: {acc:.4f}\")\n",
    "    test_acc = test_xgboost(model, X_test=X_ec_test, y_test=y_ec_test, scaler=ec_scaler, lookup_dict=ec_lookup_dict, task='ec_prediction')\n",
    "\n",
    "    \n",
    "    # Product/substrate prediction with 256-bit Morgan fingerprint\n",
    "    print(\"\\nTraining Product Prediction Model...\")\n",
    "    product_model, product_evals_result, product_true, product_pred = train_xgboost(X=X_product, y=y_product, scaler=product_scaler, train_size=2000, task=\"product_prediction\", lookup_dict=product_lookup_dict, scale_ec_factor=1)\n",
    "    print(\"\\nTraining Substrate Prediction Model...\")\n",
    "    substrate_model, substrate_evals_result, substrate_true, substrate_pred = train_xgboost(X=X_substrate, y=y_substrate, scaler=substrate_scaler, train_size=2000, task=\"substrate_prediction\", lookup_dict=substrate_lookup_dict, scale_ec_factor=1)\n",
    "\n",
    "    test_acc = test_xgboost(product_model, X_test=X_product_test, y_test=y_product_test, scaler=product_scaler, lookup_dict=product_lookup_dict, task='product_prediction', scale_ec_factor=1)\n",
    "    test_acc2 = test_xgboost(substrate_model, X_test=X_substrate_test, y_test=y_substrate_test, scaler=substrate_scaler, lookup_dict=substrate_lookup_dict, task='substrate_prediction', scale_ec_factor=1)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_PoC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
